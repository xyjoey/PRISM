@misc{berrios2023lens,
  selected={true},
  title = {Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language},
  author = {William Berrios and Gautam Mittal and Tristan Thrush and Douwe Kiela and Amanpreet Singh},
  year = {2023},
  month = jun,
  eprint = {2306.16410},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV},
  doi = {10.48550/arXiv.2306.16410},
  abstract = {We propose LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs). Our system uses a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image. We evaluate the approach on pure computer vision settings such as zero- and few-shot object recognition, as well as on vision and language problems. LENS can be applied to any off-the-shelf LLM and we find that the LLMs with LENS perform highly competitively with much bigger and much more sophisticated systems, without any multimodal training whatsoever.},
  description = {A modular approach enabling LLMs to solve computer vision tasks by reasoning over descriptive vision module outputs, achieving competitive performance without multimodal training.},
  keywords = {Computer Vision, Large Language Models, Vision-Language, Zero-Shot Learning},
  preview = {lens.png},
  code = {https://github.com/ContextualAI/lens}
}

@misc{saadfalcon2024lmunit,
  selected={true},
  title = {LMUnit: Fine-grained Evaluation with Natural Language Unit Tests},
  author = {Jon Saad-Falcon and Rajan Vivek and William Berrios and Nandita Shankar Naik and Matija Franklin and Bertie Vidgen and Amanpreet Singh and Douwe Kiela and Shikib Mehri},
  year = {2024},
  month = dec,
  eprint = {2412.13091},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  doi = {10.48550/arXiv.2412.13091},
  abstract = {As language models become integral to critical workflows, assessing their behavior remains a fundamental challenge -- human evaluation is costly and noisy, while automated metrics provide only coarse, difficult-to-interpret signals. We introduce natural language unit tests, a paradigm that decomposes response quality into explicit, testable criteria, along with a unified scoring model, LMUnit, which combines multi-objective training across preferences, direct ratings, and natural language rationales. Through controlled human studies, we show this paradigm significantly improves inter-annotator agreement and enables more effective LLM development workflows. LMUnit achieves state-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and competitive results on RewardBench. These results validate both our proposed paradigm and scoring model, suggesting a promising path forward for language model evaluation and development.},
  description = {A natural language unit testing paradigm for fine-grained LLM evaluation, achieving state-of-the-art on FLASK and BigGenBench benchmarks.},
  keywords = {Language Model Evaluation, Natural Language Processing, Benchmarking, LLM Development},
  preview = {lmunit.png},
  code = {https://github.com/ContextualAI/LMUnit}
}

@inproceedings{lui2024fairness,
  selected={true},
  title = {Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision},
  author = {Nicholas Lui and Bryan Chia and William Berrios and Candace Ross and Douwe Kiela},
  year = {2024},
  month = mar,
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {13},
  pages = {14220--14228},
  doi = {10.1609/aaai.v38i13.29333},
  abstract = {Computer vision models have been known to encode harmful biases, leading to the potentially unfair treatment of historically marginalized groups, such as people of color. However, there remains a lack of datasets balanced along demographic traits that can be used to evaluate the downstream fairness of these models. In this work, we demonstrate that diffusion models can be leveraged to create such a dataset. We first use a diffusion model to generate a large set of images depicting various occupations. Subsequently, each image is edited using inpainting to generate multiple variants, where each variant refers to a different perceived race. Using this dataset, we benchmark several vision-language models on a multi-class occupation classification task. We find that images generated with non-Caucasian labels have a significantly higher occupation misclassification rate than images generated with Caucasian labels, and that several misclassifications are suggestive of racial biases. We measure a model's downstream fairness by computing the standard deviation in the probability of predicting the true occupation label across the different identity groups. Using this fairness metric, we find significant disparities between the evaluated vision-and-language models. We hope that our work demonstrates the potential value of diffusion methods for fairness evaluations.},
  description = {Using diffusion model perturbations to create demographically balanced datasets for measuring racial bias in vision-language models.},
  keywords = {Fairness, Computer Vision, Diffusion Models, Bias Evaluation},
  preview = {fairness.jpg}
}

@misc{berrios2022brainscore,
  selected={true},
  title = {Joint Rotational Invariance and Adversarial Training of a Dual-Stream Transformer Yields State of the Art Brain-Score for Area V4},
  author = {William Berrios and Arturo Deza},
  year = {2022},
  month = mar,
  eprint = {2203.06649},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV},
  doi = {10.48550/arXiv.2203.06649},
  abstract = {Modern high-scoring models of vision in the brain score competition do not stem from Vision Transformers. However, in this paper, we provide evidence against the unexpected trend of Vision Transformers (ViT) being not perceptually aligned with human visual representations by showing how a dual-stream Transformer, a CrossViT, under a joint rotationally-invariant and adversarial optimization procedure yields 2nd place in the aggregate Brain-Score 2022 competition averaged across all visual categories, and at the time of the competition held 1st place for the highest explainable variance of area V4. In addition, our current Transformer-based model also achieves greater explainable variance for areas V4, IT and Behaviour than a biologically-inspired CNN (ResNet50) that integrates a frontal V1-like computation module. To assess the contribution of the optimization scheme with respect to the CrossViT architecture, we perform several additional experiments on differently optimized CrossViTs regarding adversarial robustness, common corruption benchmarks, mid-ventral stimuli interpretation and feature inversion. Against our initial expectations, our family of results provides tentative support for an "All roads lead to Rome" argument enforced via a joint optimization rule even for non biologically-motivated models of vision such as Vision Transformers.},
  description = {A dual-stream Vision Transformer achieving state-of-the-art Brain-Score for area V4 through joint rotational invariance and adversarial training.},
  keywords = {Computational Neuroscience, Vision Transformers, Brain-Score, Adversarial Training},
  preview = {brainscore.png},
  code = {https://github.com/williamberrios/BrainScore-Transformers}
}

@article{trelles2025bilava,
  selected={true},
  title = {BI-LAVA: Biocuration with Hierarchical Image Labelling through Active Learning and Visual Analytics},
  author = {Juan Trelles Trabucco and Andrew Wentzel and William Berrios and Hagit Shatkay and G. Elisabeta Marai},
  year = {2025},
  month = jan,
  journal = {Computer Graphics Forum},
  volume = {44},
  number = {1},
  doi = {10.1111/cgf.15261},
  abstract = {In the biomedical domain, taxonomies organize the acquisition modalities of scientific images in hierarchical structures. Such taxonomies leverage large sets of correct image labels and provide essential information about the importance of a scientific publication, which could then be used in biocuration tasks. However, the hierarchical nature of the labels, the overhead of processing images, the absence or incompleteness of labelled data and the expertise required to label this type of data impede the creation of useful datasets for biocuration. From a multi-year collaboration with biocurators and text-mining researchers, we derive an iterative visual analytics and active learning strategy to address these challenges. We implement this strategy in a system called BI-LAVA. BI-LAVA leverages a small set of image labels, a hierarchical set of image classifiers and active learning to help model builders deal with incomplete ground-truth labels, target a hierarchical taxonomy of image modalities and classify a large pool of unlabelled images. An evaluation with machine learning practitioners shows that our mixed human-machine approach successfully supports domain experts in understanding the characteristics of classes within the taxonomy, as well as validating and improving data quality in labelled and unlabelled collections.},
  description = {A visual analytics and active learning system for hierarchical biomedical image classification supporting biocuration workflows.},
  keywords = {Visual Analytics, Active Learning, Biomedical Imaging, Biocuration},
  preview = {bilava.png}
}
